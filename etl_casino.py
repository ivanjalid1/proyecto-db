#!/bin/python

"""
================================================================================
ETL CASINO - AN√ÅLISIS REGIONAL DE DEP√ìSITOS
================================================================================
Metodolog√≠a: Hefesto (Extracci√≥n ‚Üí Transformaci√≥n ‚Üí Carga)
Prop√≥sito: Procesar transacciones de casino y enriquecer con datos geogr√°ficos
Fila Aprox: 122,100 registros
Fecha: 2025-10-14
================================================================================
"""

import pandas as pd
import json
import logging
from datetime import datetime
from pathlib import Path

# ================================================================================
# CONFIGURACI√ìN DE LOGGING
# ================================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('etl_casino.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ETLCasino:
    """
    Clase que implementa el ETL para an√°lisis regional de dep√≥sitos en casino.
    Sigue la metodolog√≠a Hefesto: Extract ‚Üí Transform ‚Üí Load
    """
    
    def __init__(self, csv_path, json_path, json_pobreza_path=None):
        """Inicializa rutas de archivos"""
        self.csv_path = csv_path
        self.json_path = json_path
        self.json_pobreza_path = json_pobreza_path
        self.df_transacciones = None
        self.df_regiones = None
        self.df_pobreza = None
        self.df_procesado = None
        self.area_code_set = None
        self.area_code_to_prov = None
        logger.info("ETL inicializado")
    
    # ============================================================================
    # ETAPA 1: EXTRACCI√ìN (EXTRACT)
    # ============================================================================
    
    def extract_csv(self):
        """
        Extrae datos del CSV de transacciones del casino.
        Maneja posibles errores de lectura.
        """
        try:
            logger.info(f"Extrayendo datos de {self.csv_path}")
            self.df_transacciones = pd.read_csv(self.csv_path)
            logger.info(f"‚úì CSV extra√≠do: {len(self.df_transacciones)} filas")
            logger.info(f"  Columnas: {list(self.df_transacciones.columns)}")
            return self.df_transacciones
        except FileNotFoundError:
            logger.error(f"‚úó Archivo no encontrado: {self.csv_path}")
            raise
        except Exception as e:
            logger.error(f"‚úó Error al extraer CSV: {e}")
            raise
    
    def extract_json_regiones(self):
        """
        Extrae datos de regiones desde JSON.
        Estructura esperada: lista de objetos con areaCode, city, province.
        """
        try:
            logger.info(f"Extrayendo datos de {self.json_path}")
            with open(self.json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Convertir a DataFrame
            self.df_regiones = pd.DataFrame(data)
            logger.info(f"‚úì JSON extra√≠do: {len(self.df_regiones)} registros")
            logger.info(f"  Columnas: {list(self.df_regiones.columns)}")
            return self.df_regiones
        except FileNotFoundError:
            logger.error(f"‚úó Archivo no encontrado: {self.json_path}")
            raise
        except Exception as e:
            logger.error(f"‚úó Error al extraer JSON: {e}")
            raise

    def extract_json_pobreza(self):
        """
        Extrae datos de √≠ndices de pobreza desde JSON.
        Estructura esperada: lista de objetos con provincia, ciudad e √≠ndices socioecon√≥micos.
        """
        if not self.json_pobreza_path:
            logger.warning("‚ö† No se especific√≥ archivo de datos de pobreza, omitiendo...")
            return None

        try:
            logger.info(f"Extrayendo datos de pobreza de {self.json_pobreza_path}")
            with open(self.json_pobreza_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Convertir a DataFrame
            self.df_pobreza = pd.DataFrame(data)
            logger.info(f"‚úì JSON pobreza extra√≠do: {len(self.df_pobreza)} registros")
            logger.info(f"  Columnas: {list(self.df_pobreza.columns)}")
            return self.df_pobreza
        except FileNotFoundError:
            logger.error(f"‚úó Archivo no encontrado: {self.json_pobreza_path}")
            raise
        except Exception as e:
            logger.error(f"‚úó Error al extraer JSON pobreza: {e}")
            raise
    
    # ============================================================================
    # ETAPA 2: TRANSFORMACI√ìN (TRANSFORM)
    # ============================================================================

    def normalizar_provincia(self, provincia):
        """
        Normaliza nombres de provincias para hacer match entre diferentes fuentes.
        Elimina acentos y estandariza nombres.
        """
        if pd.isna(provincia):
            return None

        # Diccionario de normalizaci√≥n
        normalizacion = {
            'C√ìRDOBA': 'CORDOBA',
            'CORDOBA': 'CORDOBA',
            'NEUQU√âN': 'NEUQUEN',
            'NEUQUEN': 'NEUQUEN',
            'R√çO NEGRO': 'RIO NEGRO',
            'RIO NEGRO': 'RIO NEGRO',
            'TUCUM√ÅN': 'TUCUMAN',
            'TUCUMAN': 'TUCUMAN',
            'CIUDAD DE BUENOS AIRES': 'BUENOS AIRES',  # CABA se mapea a Buenos Aires
        }

        provincia_upper = str(provincia).upper().strip()
        return normalizacion.get(provincia_upper, provincia_upper)

    def transform_pobreza(self):
        """
        Transforma datos de pobreza:
        - Normaliza nombres de provincias
        - Elimina duplicados por provincia (agregando promedios si hay m√∫ltiples ciudades)
        """
        if self.df_pobreza is None:
            logger.warning("‚ö† No hay datos de pobreza para transformar")
            return None

        logger.info("üîÑ Iniciando transformaci√≥n de datos de pobreza...")

        df = self.df_pobreza.copy()

        # 1. Normalizar nombres de provincias
        df['provincia_normalizada'] = df['provincia'].apply(self.normalizar_provincia)

        # Detectar cu√°les provincias fueron normalizadas
        df['provincia_upper'] = df['provincia'].str.upper().str.strip()
        cambios = df[df['provincia_upper'] != df['provincia_normalizada']]
        sin_cambios = df[df['provincia_upper'] == df['provincia_normalizada']]

        logger.info("  ‚úì Provincias normalizadas")
        if len(cambios) > 0:
            logger.info(f"    - Modificadas: {len(cambios['provincia'].unique())} provincias")
            for _, row in cambios[['provincia', 'provincia_normalizada']].drop_duplicates().iterrows():
                logger.info(f"      ‚Ä¢ {row['provincia']:25} ‚Üí {row['provincia_normalizada']}")
        logger.info(f"    - Sin cambios: {len(sin_cambios['provincia'].unique())} provincias")

        # Limpiar columnas temporales
        df = df.drop(columns=['provincia_upper'])

        # 2. Para provincias con m√∫ltiples aglomerados, calculamos promedio ponderado por poblaci√≥n
        # (esto es importante para Buenos Aires que tiene CABA + GBA)
        columnas_indices = [
            'indice_pobreza_personas', 'indice_pobreza_hogares',
            'indice_indigencia_personas', 'indice_indigencia_hogares',
            'ingreso_promedio_familia', 'canasta_basica_total',
            'brecha_pobreza_pct'
        ]

        # Agrupar por provincia y calcular promedio ponderado por poblaci√≥n
        df_agrupado = df.groupby('provincia_normalizada').apply(
            lambda x: pd.Series({
                'indice_pobreza_personas': (x['indice_pobreza_personas'] * x['poblacion_estimada']).sum() / x['poblacion_estimada'].sum(),
                'indice_pobreza_hogares': (x['indice_pobreza_hogares'] * x['poblacion_estimada']).sum() / x['poblacion_estimada'].sum(),
                'indice_indigencia_personas': (x['indice_indigencia_personas'] * x['poblacion_estimada']).sum() / x['poblacion_estimada'].sum(),
                'indice_indigencia_hogares': (x['indice_indigencia_hogares'] * x['poblacion_estimada']).sum() / x['poblacion_estimada'].sum(),
                'ingreso_promedio_familia': (x['ingreso_promedio_familia'] * x['poblacion_estimada']).sum() / x['poblacion_estimada'].sum(),
                'canasta_basica_total': (x['canasta_basica_total'] * x['poblacion_estimada']).sum() / x['poblacion_estimada'].sum(),
                'brecha_pobreza_pct': (x['brecha_pobreza_pct'] * x['poblacion_estimada']).sum() / x['poblacion_estimada'].sum(),
                'poblacion_estimada': x['poblacion_estimada'].sum()
            })
        ).reset_index()

        logger.info(f"  ‚úì Datos agregados por provincia: {len(df_agrupado)} provincias √∫nicas")

        self.df_pobreza = df_agrupado
        return df_agrupado

    def extract_area_code(self, phone):
        """
        Extrae el c√≥digo de √°rea usando el conjunto de c√≥digos reales (self.area_code_set).
        Intenta coincidencias de 4, 3 y 2 d√≠gitos (longest-first).
        Devuelve el c√≥digo como string (o None si no hay match).
        """
        try:
            phone_str = str(phone).strip()
            if phone_str.startswith('+54'):
                phone_str = phone_str[3:]
            # Si es celular con '9' despu√©s del +54, quitar ese 9
            if phone_str.startswith('9'):
                phone_str = phone_str[1:]
            # Quitar ceros leading (por si vienen mal formateados)
            phone_str = phone_str.lstrip('0')

            # Si no tenemos el set preparado, hacer un fallback simple (2-4 d√≠gitos)
            if not hasattr(self, 'area_code_set') or not self.area_code_set:
                logger.fatal("no existe el set que mapea codigos de area con las provincias, es necesario para el programa")
                exit(2)

            # Intentar match con los c√≥digos conocidos (longest-first)
            for length in (4, 3, 2):
                if len(phone_str) >= length:
                    candidate = phone_str[:length]
                    if candidate in self.area_code_set:
                        return candidate

            # Si no hay match expl√≠cito, devolvemos None (as√≠ queda NaN y lo pod√©s auditar)
            return 'DESCONOCIDO'

        except Exception as e:
            logger.warning(f"Error extrayendo √°rea de {phone}: {e}")
            return None

    def transform_transacciones(self):
        """
        Transforma y limpia datos de transacciones:
        - Normaliza nombres de columnas
        - Extrae c√≥digo de √°rea
        - Convierte tipos de dato
        - Valida estados
        """
        logger.info("üîÑ Iniciando transformaci√≥n de transacciones...")
        
        df = self.df_transacciones.copy()
        
        # 1. Normalizar nombres de columnas
        df.columns = df.columns.str.lower().str.strip()
        logger.info("  ‚úì Columnas normalizadas a min√∫sculas")
        
        # 2. Filtrar y eliminar tel√©fonos que no empiezan por '+54'
        mask_valid_phone = df['phone'].notna() & df['phone'].astype(str).str.strip().str.startswith('+54')
        invalid_count = len(df) - mask_valid_phone.sum()
        if invalid_count > 0:
            logger.warning(f"  ‚ö† Eliminando {invalid_count} filas cuyo tel√©fono no empieza con '+54'")
        df = df[mask_valid_phone].copy()

        # 3. Extraer c√≥digo de √°rea (solo sobre tel√©fonos v√°lidos)
        df['area_code'] = df['phone'].apply(self.extract_area_code)
        logger.info("  ‚úì C√≥digos de √°rea extra√≠dos (solo tel√©fonos +54)")


        # 3. Convertir fecha a datetime
        df['fecha'] = pd.to_datetime(df['fecha'], errors='coerce')

        # Eliminar filas con fechas nulas
        filas_antes_fecha = len(df)
        df = df[df['fecha'].notna()].copy()
        filas_eliminadas_fecha = filas_antes_fecha - len(df)
        if filas_eliminadas_fecha > 0:
            logger.warning(f"  ‚ö† Eliminadas {filas_eliminadas_fecha} filas con fecha nula")

        logger.info("  ‚úì Fechas convertidas a datetime")
        
        # 4. Convertir monto a num√©rico
        df['monto'] = pd.to_numeric(df['monto'], errors='coerce')

        # Validar y eliminar montos inv√°lidos (nulos, negativos o cero)
        filas_antes_monto = len(df)
        df = df[(df['monto'].notna()) & (df['monto'] > 0)].copy()
        filas_eliminadas_monto = filas_antes_monto - len(df)
        if filas_eliminadas_monto > 0:
            logger.warning(f"  ‚ö† Eliminadas {filas_eliminadas_monto} filas con monto inv√°lido (nulo, negativo o cero)")

        logger.info("  ‚úì Montos convertidos a num√©rico y validados")
        
        # 5. Normalizar estado
        df['estado'] = df['estado'].str.upper()
        logger.info("  ‚úì Estados normalizados")
        
        # 6. Normalizar tipo
        df['tipo'] = df['tipo'].str.upper()
        logger.info("  ‚úì Tipos normalizados")
        
        # 7. Detectar filas problem√°ticas
        filas_incompletas = df[df['area_code'].isna()].shape[0]
        if filas_incompletas > 0:
            logger.warning(f"  ‚ö† {filas_incompletas} filas con √°rea_code inv√°lido")
        
        self.df_transacciones = df
        return df
    
    def transform_regiones(self):
        """
        Transforma datos de regiones:
        - Normaliza areaCode
        - Normaliza nombres de provincia y ciudad
        - Elimina duplicados (manteniendo el primero)
        """
        logger.info("üîÑ Iniciando transformaci√≥n de regiones...")
        
        df = self.df_regiones.copy()
        
        # 1. Convertir areaCode a string y remover leading zeros
        df['areaCode'] = df['areaCode'].astype(str).str.strip()
        logger.info("  ‚úì C√≥digos de √°rea estandarizados")
        
        # 2. Normalizar nombres
        df['province'] = df['province'].str.upper().str.strip()
        df['city'] = df['city'].str.upper().str.strip()
        logger.info("  ‚úì Provincias y ciudades normalizadas")
        
        # 3. Eliminar duplicados (mantener primer registro)
        filas_antes = len(df)
        df = df.drop_duplicates(subset=['areaCode'], keep='first')
        filas_despues = len(df)
        logger.info(f"  ‚úì Duplicados eliminados: {filas_antes - filas_despues} registros")
        
        # 4. Crear clave de b√∫squeda
        df['region_key'] = df['areaCode']

        # Normalizar areaCode sin ceros iniciales (por si vinieron como "034" o "011")
        df['areaCode'] = df['areaCode'].str.lstrip('0')

        # Crear set y diccionario para matching r√°pido
        self.area_code_set = set(df['areaCode'].astype(str).unique())
        # mapping area_code -> provincia (normalizada)
        self.area_code_to_prov = df.set_index('areaCode')['province'].to_dict()

        logger.info(f"  ‚úì Diccionario de c√≥digos de √°rea creado ({len(self.area_code_set)} c√≥digos)")

        
        self.df_regiones = df
        return df
    
    def merge_datos(self):
        """
        Combina datos de transacciones con regiones mediante JOIN.
        Matching por c√≥digo de √°rea telef√≥nico.
        """
        logger.info("üîó Realizando JOIN entre transacciones y regiones...")

        # Asegurar que las transacciones tengan area_code usando el diccionario creado
        if 'area_code' not in self.df_transacciones.columns or self.df_transacciones['area_code'].isnull().all():
            logger.info("  ‚úì Extrayendo area_code de transacciones usando diccionario de regiones")

            self.df_transacciones['area_code'] = self.df_transacciones['phone'].apply(self.extract_area_code)
            missing = self.df_transacciones['area_code'].isna().sum()

            if missing > 0:
                logger.warning(f"  ‚ö† {missing} filas sin match de area_code tras intentar con el diccionario")

        
        # Preparar DataFrames
        trans = self.df_transacciones.copy()
        regiones = self.df_regiones[['areaCode', 'province', 'city']].copy()
        regiones.columns = ['area_code', 'provincia', 'ciudad']
        
        # JOIN
        df_merge = trans.merge(
            regiones,
            on='area_code',
            how='left'
        )
        
        # Estad√≠sticas del JOIN
        matches = df_merge['provincia'].notna().sum()
        no_matches = df_merge['provincia'].isna().sum()

        logger.info(f"  ‚úì JOIN completado")
        logger.info(f"    - Registros con regi√≥n: {matches} ({100*matches/len(df_merge):.1f}%)")
        logger.info(f"    - Registros sin regi√≥n: {no_matches} ({100*no_matches/len(df_merge):.1f}%)")

        # Eliminar filas donde la provincia es nula (no se encontr√≥ match)
        filas_antes_provincia = len(df_merge)
        df_merge = df_merge[df_merge['provincia'].notna()].copy()
        filas_eliminadas_provincia = filas_antes_provincia - len(df_merge)
        if filas_eliminadas_provincia > 0:
            logger.warning(f"  ‚ö† Eliminadas {filas_eliminadas_provincia} filas sin match de provincia")

        # JOIN con datos de pobreza (si est√°n disponibles)
        if self.df_pobreza is not None:
            logger.info("üîó Realizando JOIN con datos de pobreza...")

            # Normalizar provincia en transacciones para hacer match
            df_merge['provincia_normalizada'] = df_merge['provincia'].apply(self.normalizar_provincia)

            # JOIN con datos de pobreza
            df_merge = df_merge.merge(
                self.df_pobreza,
                on='provincia_normalizada',
                how='left'
            )

            # Estad√≠sticas del JOIN
            matches_pobreza = df_merge['indice_pobreza_personas'].notna().sum()
            no_matches_pobreza = df_merge['indice_pobreza_personas'].isna().sum()

            logger.info(f"  ‚úì JOIN con pobreza completado")
            logger.info(f"    - Registros con datos de pobreza: {matches_pobreza} ({100*matches_pobreza/len(df_merge):.1f}%)")
            if no_matches_pobreza > 0:
                logger.warning(f"    - Registros sin datos de pobreza: {no_matches_pobreza} ({100*no_matches_pobreza/len(df_merge):.1f}%)")
                # Mostrar qu√© provincias no tienen match
                provincias_sin_match = df_merge[df_merge['indice_pobreza_personas'].isna()]['provincia_normalizada'].unique()
                logger.warning(f"    - Provincias sin datos: {list(provincias_sin_match)}")

        self.df_transacciones = df_merge
        return df_merge
    
    def agregar_campos_derivados(self):
        """
        Crea campos calculados para an√°lisis:
        - A√±o, mes, d√≠a de transacci√≥n
        - Clasificaci√≥n de monto (bajo/medio/alto)
        - Flag de dep√≥sito exitoso
        """
        logger.info("‚ûï Agregando campos derivados...")
        
        df = self.df_transacciones.copy()
        
        # 1. Componentes de fecha
        df['anio'] = df['fecha'].dt.year
        df['mes'] = df['fecha'].dt.month
        df['dia'] = df['fecha'].dt.day
        df['dia_semana'] = df['fecha'].dt.day_name()
        df['hora'] = df['fecha'].dt.hour
        logger.info("  ‚úì Componentes de fecha extra√≠dos")    
        
        self.df_transacciones = df
        return df
    
    def validar_calidad(self):
        """
        Realiza validaciones de calidad de datos.
        Registra anomal√≠as detectadas.
        """
        logger.info("‚úÖ Realizando validaciones de calidad...")

        df = self.df_transacciones

        # 1. Validaci√≥n cr√≠tica: Campos obligatorios no pueden ser nulos
        campos_criticos = ['provincia', 'ciudad', 'fecha', 'monto', 'area_code']
        filas_antes_validacion = len(df)

        for campo in campos_criticos:
            if campo in df.columns:
                nulos_campo = df[campo].isna().sum()
                if nulos_campo > 0:
                    logger.error(f"  ‚ùå CR√çTICO: {nulos_campo} valores nulos en campo '{campo}'")
                    df = df[df[campo].notna()].copy()

        filas_eliminadas_validacion = filas_antes_validacion - len(df)
        if filas_eliminadas_validacion > 0:
            logger.warning(f"  ‚ö† Eliminadas {filas_eliminadas_validacion} filas adicionales por validaci√≥n de campos cr√≠ticos")
            self.df_transacciones = df

        # 2. Nulos en campos no cr√≠ticos
        nulos = df.isnull().sum()
        if nulos.sum() > 0:
            logger.warning("  ‚ö† Campos con valores nulos detectados:")
            for col, count in nulos[nulos > 0].items():
                logger.warning(f"    - {col}: {count} ({100*count/len(df):.2f}%)")
        else:
            logger.info("  ‚úì No hay campos con valores nulos")

        # 3. Validar montos
        montos_neg = (df['monto'] < 0).sum()
        montos_cero = (df['monto'] == 0).sum()
        if montos_neg > 0:
            logger.error(f"  ‚ùå CR√çTICO: {montos_neg} montos negativos detectados")
        if montos_cero > 0:
            logger.error(f"  ‚ùå CR√çTICO: {montos_cero} montos en cero detectados")
        if montos_neg == 0 and montos_cero == 0:
            logger.info(f"  ‚úì Todos los montos son v√°lidos (> 0)")

        # 4. Estados desconocidos
        estados_unicos = df['estado'].unique()
        logger.info(f"  ‚Ñπ Estados √∫nicos: {estados_unicos}")

        # 5. Cobertura geogr√°fica
        cobertura = (df['provincia'].notna().sum() / len(df)) * 100
        logger.info(f"  ‚Ñπ Cobertura geogr√°fica: {cobertura:.1f}%")

        # 6. Rango de fechas
        if df['fecha'].notna().any():
            fecha_min = df['fecha'].min()
            fecha_max = df['fecha'].max()
            logger.info(f"  ‚Ñπ Rango de fechas: {fecha_min} a {fecha_max}")

        # 7. Estad√≠sticas de montos
        logger.info(f"  ‚Ñπ Monto m√≠nimo: ${df['monto'].min():,.2f}")
        logger.info(f"  ‚Ñπ Monto m√°ximo: ${df['monto'].max():,.2f}")
        logger.info(f"  ‚Ñπ Monto promedio: ${df['monto'].mean():,.2f}")

        logger.info("‚úì Validaciones completadas")
    
    # ============================================================================
    # ETAPA 3: CARGA (LOAD)
    # ============================================================================
    
    def load_csv(self, output_path='casino_procesado.csv'):
        """Exporta datos procesados a CSV"""
        try:
            logger.info(f"üíæ Exportando datos a {output_path}")

            # Seleccionar columnas relevantes (incluir datos de pobreza si existen)
            cols_export = [
                'username', 'phone', 'area_code', 'provincia', 'ciudad',
                'monto', 'fecha', 'anio', 'mes', 'dia', 'hora',
                'estado', 'tipo', 'dia_semana'
            ]

            # Agregar columnas de pobreza si est√°n disponibles
            if 'indice_pobreza_personas' in self.df_transacciones.columns:
                cols_pobreza = [
                    'indice_pobreza_personas', 'indice_pobreza_hogares',
                    'indice_indigencia_personas', 'indice_indigencia_hogares',
                    'ingreso_promedio_familia', 'canasta_basica_total',
                    'brecha_pobreza_pct', 'poblacion_estimada'
                ]
                cols_export.extend(cols_pobreza)
                logger.info(f"  ‚úì Incluyendo {len(cols_pobreza)} columnas de √≠ndices socioecon√≥micos")

            # Filtrar solo columnas que existan
            cols_export = [col for col in cols_export if col in self.df_transacciones.columns]

            df_export = self.df_transacciones[cols_export].copy()
            df_export.to_csv(output_path, index=False, encoding='utf-8')

            logger.info(f"‚úì Archivo guardado: {output_path}")
            logger.info(f"  - Filas: {len(df_export)}")
            logger.info(f"  - Columnas: {len(df_export.columns)}")
            return output_path
        except Exception as e:
            logger.error(f"‚úó Error al exportar CSV: {e}")
            raise
    
    def load_parquet(self, output_path='casino_procesado.parquet'):
        """Exporta datos procesados a Parquet (formato optimizado)"""
        try:
            logger.info(f"üíæ Exportando datos a {output_path}")
            self.df_transacciones.to_parquet(output_path, index=False)
            logger.info(f"‚úì Archivo Parquet guardado: {output_path}")
            return output_path
        except ImportError:
            logger.warning("‚ö† pyarrow no instalado. Instala con: pip install pyarrow")
        except Exception as e:
            logger.error(f"‚úó Error al exportar Parquet: {e}")
            raise
    
    # ============================================================================
    # EJECUCI√ìN DEL ETL COMPLETO
    # ============================================================================
    
    def ejecutar(self):
        """Ejecuta todas las etapas del ETL"""
        inicio = datetime.now()
        logger.info("=" * 80)
        logger.info("INICIANDO ETL CASINO")
        logger.info("=" * 80)
        
        try:
            # EXTRACT
            logger.info("\nüì• ETAPA 1: EXTRACCI√ìN")
            self.extract_csv()
            self.extract_json_regiones()
            self.extract_json_pobreza()

            # TRANSFORM
            logger.info("\nüîÑ ETAPA 2: TRANSFORMACI√ìN")
            self.transform_regiones()
            if self.df_pobreza is not None:
                self.transform_pobreza()
            self.transform_transacciones()
            self.merge_datos()
            self.agregar_campos_derivados()
            self.validar_calidad()
            
            # LOAD
            logger.info("\nüì§ ETAPA 3: CARGA")
            self.load_csv('./datos_salida/casino_procesado.csv')
            self.load_parquet('./datos_salida/casino_procesado.parquet')
            
            # RESUMEN
            tiempo_total = (datetime.now() - inicio).total_seconds()
            logger.info("\n" + "=" * 80)
            logger.info(f"‚úÖ ETL COMPLETADO EXITOSAMENTE en {tiempo_total:.2f} segundos")
            logger.info("=" * 80)
            
            return self.df_transacciones
            
        except Exception as e:
            logger.error(f"\n‚ùå ETL FALL√ì: {e}")
            raise

# ================================================================================
# EJECUCI√ìN
# ================================================================================

if __name__ == "__main__":
    # Configurar rutas
    csv_file = "./datos_entrada/casino_transacciones.csv"           # Tu archivo CSV
    json_file = "./datos_entrada/regiones_argentina.json"           # Tu archivo JSON regiones
    json_pobreza_file = "./datos_entrada/datos_pobreza.json"        # Tu archivo JSON pobreza

    # Crear instancia y ejecutar ETL
    etl = ETLCasino(csv_file, json_file, json_pobreza_file)
    df_final = etl.ejecutar()
    
    # Mostrar muestra de datos finales
    logger.info("\nüìä MUESTRA DE DATOS PROCESADOS:")
    logger.info(df_final.head(10).to_string())
    
    # Estad√≠sticas finales
    logger.info("\nüìà ESTAD√çSTICAS FINALES:")
    logger.info(f"Total dep√≥sitos: {len(df_final):,}")
    logger.info(f"Monto total: ${df_final['monto'].sum():,.2f}")
    logger.info(f"Monto promedio: ${df_final['monto'].mean():,.2f}")
    logger.info(f"Provincias: {df_final['provincia'].nunique()}")
    logger.info(f"Ciudades: {df_final['ciudad'].nunique()}")
    logger.info(f"\nDepositos por provincia:\n{df_final.groupby('provincia')['monto'].agg(['count', 'sum', 'mean']).round(2)}")
