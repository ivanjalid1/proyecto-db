# ğŸ° ETL Casino - GuÃ­a Completa

## ğŸ“‹ Tabla de Contenidos

1. [DescripciÃ³n General](#descripciÃ³n-general)
2. [Requisitos](#requisitos)
3. [InstalaciÃ³n](#instalaciÃ³n)
4. [Estructura de Archivos](#estructura-de-archivos)
5. [GuÃ­a de Uso](#guÃ­a-de-uso)
6. [Ejemplos](#ejemplos)
7. [Preguntas de AnÃ¡lisis](#preguntas-de-anÃ¡lisis)
8. [ResoluciÃ³n de Problemas](#resoluciÃ³n-de-problemas)

---

## DescripciÃ³n General

Este proyecto implementa un **proceso ETL completo (Extract-Transform-Load)** usando la metodologÃ­a **Hefesto** para procesar transacciones de un casino online y enriquecerlas con datos geogrÃ¡ficos por regiÃ³n argentina.

### Objetivos

âœ… Procesar ~122,100 transacciones de depÃ³sitos  
âœ… Enriquecer datos con informaciÃ³n geogrÃ¡fica (provincia, ciudad)  
âœ… Validar y limpiar datos inconsistentes  
âœ… Generar anÃ¡lisis regionales ejecutivos  

### TecnologÃ­a

- **Lenguaje:** Python 3.8+
- **LibrerÃ­as:** pandas, numpy, pyarrow (opcional)
- **Formato de datos:** CSV, JSON â†’ Parquet, Excel

---

## Requisitos

### Hardware MÃ­nimo
- RAM: 2 GB
- Espacio: 500 MB
- Procesador: Dual-core

### Software
- Python 3.8 o superior
- pip (gestor de paquetes)

---

## InstalaciÃ³n

### 1. Clonar o descargar el proyecto

```bash
cd tu_proyecto_etl
```

### 2. Crear entorno virtual (recomendado)

```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 3. Instalar dependencias

```bash
pip install pandas numpy pyarrow openpyxl
```

---

## Estructura de Archivos

```
proyecto_etl/
â”‚
â”œâ”€â”€ ğŸ“„ etl_casino.py               # Script principal ETL
â”œâ”€â”€ ğŸ“„ analytics_casino.py         # Script de anÃ¡lisis
â”œâ”€â”€ ğŸ“„ README.md                   # Este archivo
â”‚
â”œâ”€â”€ ğŸ“ datos_entrada/
â”‚   â”œâ”€â”€ casino_transacciones.csv   # Datos crudos (122K filas)
â”‚   â”œâ”€â”€ regiones_argentina.json    # Mapeo geogrÃ¡fico
|   â””â”€â”€ datos_pobreza.json         # Mapeo de pobreza (no estÃ¡ implementado)
â”‚
â”œâ”€â”€ ğŸ“ datos_salida/
â”‚   â”œâ”€â”€ casino_procesado.csv       # Datos limpios (CSV)
â”‚   â”œâ”€â”€ casino_procesado.parquet   # Datos comprimidos (Parquet)
â”‚   â”œâ”€â”€ casino_reportes.xlsx       # Reportes en Excel
â”‚   â””â”€â”€ etl_casino.log             # Log detallado
â”‚
â””â”€â”€ ğŸ“ documentacion/
    â”œâ”€â”€ METODOLOGIA.md             # Detalle de transformaciones
    â”œâ”€â”€ ANALISIS.md                # GuÃ­a de anÃ¡lisis
    â””â”€â”€ guia_ejecucion             # Paso a paso de la ejecuciÃ³n
```

---

## GuÃ­a de Uso

### Paso 1: Preparar Datos de Entrada

**casino_transacciones.csv:**
```csv
username,phone,monto,fecha,estado,tipo
marco068690,+5493518612669,500,2025-10-14 19:30:58.954,SUCCESS,DEPOSIT
rica07134,+5491167935855,2500,2025-10-14 19:30:52.235,SUCCESS,DEPOSIT
```

**regiones_argentina.json:**
```json
[
  {
    "areaCode": "2345",
    "city": "25 DE MAYO",
    "province": "Buenos Aires",
    "carrier": "TELECOM PERSONAL S.A.",
    "numberType": "M"
  },
  ...
]
```

### Paso 2: Ejecutar ETL

**OpciÃ³n A: EjecuciÃ³n Simple**

```python
from etl_casino import ETLCasino

# Crear instancia
etl = ETLCasino(
    csv_path="datos_entrada/casino_transacciones.csv",
    json_path="datos_entrada/regiones_argentina.json"
)

# Ejecutar todas las etapas
df_procesado = etl.ejecutar()
```

**OpciÃ³n B: Desde lÃ­nea de comandos**

```bash
python etl_casino.py
```

### Paso 3: Ejecutar AnÃ¡lisis

```python
from analytics_casino import AnalyticsCasino

# Crear instancia con datos procesados
analytics = AnalyticsCasino('datos_salida/casino_procesado.csv')

# Generar reporte ejecutivo completo
analytics.generar_reporte_ejecutivo()

# O anÃ¡lisis especÃ­ficos
analytics.analisis_por_provincia()
analytics.usuarios_por_volume()
```

### Paso 4: Exportar Reportes a Excel

```python
from analytics_casino import exportar_reportes_excel

exportar_reportes_excel(
    csv_path='datos_salida/casino_procesado.csv',
    output_path='datos_salida/casino_reportes.xlsx'
)
```

---

## Ejemplos

### Ejemplo 1: Listar Top Provincias

```python
import pandas as pd

df = pd.read_csv('datos_salida/casino_procesado.csv')

# Top 10 provincias por monto total
top_provincias = df[df['tipo'] == 'DEPOSIT'].groupby('provincia').agg({
    'monto': ['count', 'sum', 'mean']
}).round(2).sort_values(('monto', 'sum'), ascending=False).head(10)

print(top_provincias)
```

**Output:**
```
provincia              
Buenos Aires       Transacciones: 45,234 | Total: $2,340,500 | Promedio: $51.68
CÃ³rdoba            Transacciones: 12,456 | Total: $780,200 | Promedio: $62.60
Santa Fe           Transacciones: 8,923 | Total: $450,150 | Promedio: $50.47
```

### Ejemplo 2: Usuarios de Mayor DepÃ³sito por RegiÃ³n

```python
# Top 5 usuarios por provincia
for provincia in df['provincia'].dropna().unique()[:5]:
    usuarios = df[
        (df['provincia'] == provincia) & 
        (df['tipo'] == 'DEPOSIT')
    ].groupby('username')['monto'].sum().nlargest(5)
    
    print(f"\nğŸ”¹ {provincia}")
    print(usuarios)
```

### Ejemplo 3: AnÃ¡lisis Temporal

```python
# EvoluciÃ³n diaria de depÃ³sitos
df['fecha_dia'] = pd.to_datetime(df['fecha']).dt.date

depositos_diarios = df[df['tipo'] == 'DEPOSIT'].groupby('fecha_dia').agg({
    'monto': ['count', 'sum']
}).round(2)

depositos_diarios.columns = ['Transacciones', 'Total']
print(depositos_diarios.tail(30))  # Ãšltimos 30 dÃ­as
```

### Ejemplo 4: SegmentaciÃ³n de Usuarios

```python
# Â¿QuÃ© % de usuarios genera el 80% de los depÃ³sitos?
usuarios_total = df[df['tipo'] == 'DEPOSIT'].groupby('username')['monto'].sum()
usuarios_total_ord = usuarios_total.sort_values(ascending=False)

acumulativo = usuarios_total_ord.cumsum()
pct_acumulativo = (acumulativo / acumulativo.iloc[-1] * 100)

usuarios_80 = (pct_acumulativo <= 80).sum()
pct_usuarios = (usuarios_80 / len(usuarios_total) * 100)

print(f"El 80% de depÃ³sitos viene de {usuarios_80} usuarios ({pct_usuarios:.1f}%)")
```

---

## Preguntas de AnÃ¡lisis

Estas preguntas pueden responderse con los datos procesados:

### ğŸ“Š Preguntas por RegiÃ³n

1. **Â¿QuÃ© provincias generan mÃ¡s depÃ³sitos?**
2. **Â¿CuÃ¡l es el monto promedio por provincia?**
3. **Â¿QuÃ© ciudades tienen mÃ¡s usuarios activos?**
4. **Â¿QuÃ© operador telefÃ³nico tiene mÃ¡s usuarios?**

### ğŸ‘¥ Preguntas por Usuario

5. **Â¿QuiÃ©nes son los top 100 usuarios depositantes?**
6. **Â¿CuÃ¡l es el patrÃ³n de deposito tÃ­pico por usuario?**
7. **Â¿QuÃ© porcentaje de usuarios deposita regularmente?**
8. **Â¿Hay concentraciÃ³n en pocos usuarios?** (Pareto 80/20)

### â° Preguntas Temporales

9. **Â¿En quÃ© horarios hay mÃ¡s depÃ³sitos?**
10. **Â¿QuÃ© dÃ­as de la semana son mÃ¡s activos?**
11. **Â¿Hay tendencia creciente o decreciente?**
12. **Â¿Hay patrones estacionales?**

### ğŸ’° Preguntas Financieras

13. **Â¿CuÃ¡l es la distribuciÃ³n de montos?**
14. **Â¿QuÃ© rango de monto es mÃ¡s comÃºn?**
15. **Â¿Hay outliers o anomalÃ­as?**
16. **Â¿CuÃ¡l es la tasa de Ã©xito por tipo de transacciÃ³n?**

---

## ResoluciÃ³n de Problemas

### âŒ Error: "No module named 'pandas'"

**SoluciÃ³n:**
```bash
pip install pandas
```

### âŒ Error: "FileNotFoundError: casino_transacciones.csv"

**Verificar:**
- âœ“ El archivo existe en la ruta especificada
- âœ“ La ruta es correcta (usar rutas absolutas si es necesario)
- âœ“ Permisos de lectura en el archivo

**SoluciÃ³n:**
```python
import os
print(os.path.exists("datos_entrada/casino_transacciones.csv"))
```

### âŒ Error: "MemoryError" con datos grandes

**Problema:** Dataset muy grande para RAM disponible

**Soluciones:**
1. Procesar en chunks:
```python
chunks = pd.read_csv("datos.csv", chunksize=10000)
for chunk in chunks:
    # procesar chunk
```

2. Usar Parquet en lugar de CSV:
```python
df = pd.read_parquet("datos.parquet")
```

3. Aumentar RAM o usar mÃ¡quina mÃ¡s potente

### âš ï¸ Advertencia: "CÃ³digos de Ã¡rea sin match"

**Causa:** NÃºmeros telefÃ³nicos cuyo cÃ³digo de Ã¡rea no estÃ¡ en el JSON

**VerificaciÃ³n:**
```python
sin_region = df[df['provincia'].isna()]
print(sin_region['area_code'].value_counts())
```

**SoluciÃ³n:** Actualizar JSON con cÃ³digos faltantes o usar valor por defecto.

### âš ï¸ Advertencia: "Fechas con formato inconsistente"

**VerificaciÃ³n:**
```python
df['fecha'] = pd.to_datetime(df['fecha'], errors='coerce')
print(df[df['fecha'].isna()])  # Mostrar fechas con problemas
```

---

## ğŸ“š DocumentaciÃ³n TÃ©cnica

### Etapa 1: EXTRACT (ExtracciÃ³n)

**Datos de entrada:**
- CSV: 122,100 registros Ã— 6 columnas
- JSON: ~18,450 mapeos de Ã¡rea â†’ regiÃ³n

**Tiempo:** ~2-3 segundos

### Etapa 2: TRANSFORM (TransformaciÃ³n)

**Transformaciones aplicadas:**

| TransformaciÃ³n | Entrada | Salida |
|---|---|---|
| ExtracciÃ³n de Ã¡rea | `+5493518612669` | `351` |
| NormalizaciÃ³n de fecha | `2025/01/15`, `15-02-2025` | `2025-01-15` |
| NormalizaciÃ³n de moneda | `USD`, `ars`, `ARS` | `USD`, `ARS` |
| Limpieza de nombres | `Juan`, `NULL` | `Juan`, `N/A` |
| JOIN por Ã¡rea | TransacciÃ³n + JSON | + provincia, ciudad |
| Campos derivados | fecha | anio, mes, dia, hora |

**Tiempo:** ~5-8 segundos

**Validaciones:**
- âœ“ Datos nulos detectados
- âœ“ Montos negativos alertados
- âœ“ Cobertura geogrÃ¡fica calculada
- âœ“ Estados desconocidos reportados

### Etapa 3: LOAD (Carga)

**Formatos de salida:**

1. **CSV Limpio** (45 MB)
   - Legible en Excel, SQL, etc.
   - Lento para queries grandes

2. **Parquet** (9 MB)
   - Comprimido 5x
   - Lectura 10x mÃ¡s rÃ¡pida
   - Ideal para BI

**Tiempo:** ~2-3 segundos

**Total ETL:** ~10-15 segundos

---

## ğŸ” VerificaciÃ³n Post-ETL

DespuÃ©s de ejecutar el ETL, verificar:

### 1. Integridad de Datos

```python
df = pd.read_csv('casino_procesado.csv')

# Verificar ausencia de nulos en campos crÃ­ticos
print(df[['provincia', 'monto', 'fecha']].isnull().sum())

# Verificar rangos de valores
print(f"Monto min: {df['monto'].min()}, max: {df['monto'].max()}")
print(f"Fecha min: {df['fecha'].min()}, max: {df['fecha'].max()}")
```

### 2. Calidad de JOIN

```python
# Porcentaje de registros con regiÃ³n
cobertura = df['provincia'].notna().sum() / len(df) * 100
print(f"Cobertura geogrÃ¡fica: {cobertura:.1f}%")

# Registros sin match
print(df[df['provincia'].isna()][['phone', 'area_code']].head())
```

### 3. DistribuciÃ³n de Datos

```python
# Provincias cubierto
print(f"Provincias: {df['provincia'].nunique()}")
print(f"Ciudades: {df['ciudad'].nunique()}")
print(f"Usuarios: {df['username'].nunique()}")

# Top 5 provincias
print(df.groupby('provincia')['monto'].sum().nlargest(5))
```

---

## ğŸ“ˆ MÃ©tricas Clave

| MÃ©trica | Valor Esperado |
|---------|---|
| Total de registros | ~122,100 |
| Registros con regiÃ³n | ~85-95% |
| Tiempo de ejecuciÃ³n | 10-15 seg |
| TamaÃ±o salida CSV | ~45 MB |
| TamaÃ±o salida Parquet | ~9 MB |
| Provincias identificadas | 24 |
| Ciudades Ãºnicas | 1,500+ |
| Usuarios Ãºnicos | 50,000+ |

---

## ğŸ“ MetodologÃ­a Hefesto

La metodologÃ­a Hefesto estructura el ETL en 3 fases:

### 1ï¸âƒ£ EXTRACT (ExtracciÃ³n)
- Conectar a fuentes de datos
- Validar disponibilidad
- Registrar inicio

### 2ï¸âƒ£ TRANSFORM (TransformaciÃ³n)
- Limpiar inconsistencias
- Normalizar formatos
- Enriquecer datos
- Validar calidad

### 3ï¸âƒ£ LOAD (Carga)
- Exportar a formato final
- Verificar integridad
- Generar logs

**CaracterÃ­sticas clave:**
- âœ… Rastreabilidad completa (logging)
- âœ… Validaciones en cada etapa
- âœ… Manejo de errores robusto
- âœ… DocumentaciÃ³n automÃ¡tica

---

## ğŸ’¡ Tips para AnÃ¡lisis

### AnÃ¡lisis RÃ¡pido

```python
import pandas as pd

df = pd.read_csv('casino_procesado.csv')

# 1. Top 10 provincias
print(df.groupby('provincia')['monto'].sum().nlargest(10))

# 2. Usuarios de alto valor
usuarios_alto = df[df['tipo'] == 'DEPOSIT'].groupby('username')['monto'].sum().nlargest(100)
print(usuarios_alto)

# 3. Patrones horarios
print(df.groupby('hora')['monto'].sum().sort_values(ascending=False))

# 4. AnÃ¡lisis de Ã©xito
print(f"Tasa de Ã©xito: {df['es_exitoso'].mean()*100:.1f}%")
```

### AnÃ¡lisis Avanzado

```python
# AnÃ¡lisis de Pareto - Â¿QuÃ© % de usuarios genera el 80% de ingresos?
usuarios_depositos = df[df['tipo']=='DEPOSIT'].groupby('username')['monto'].sum().sort_values(ascending=False)
cumsum = usuarios_depositos.cumsum()
pct_cumsum = cumsum / cumsum.iloc[-1]
usuarios_80pct = (pct_cumsum <= 0.8).sum()
print(f"Usuarios que generan el 80%: {usuarios_80pct} ({usuarios_80pct/len(usuarios_depositos)*100:.1f}%)")

# SegmentaciÃ³n por provincia y rango
segmentos = df[df['tipo']=='DEPOSIT'].groupby(['provincia', 'rango_monto']).agg({
    'monto': ['count', 'sum', 'mean']
}).round(2)
print(segmentos)
```

---

## ğŸ“ Soporte

### Preguntas Frecuentes

**P: Â¿CuÃ¡nto tiempo toma procesar 122K registros?**
R: Aproximadamente 10-15 segundos en una mÃ¡quina moderna.

**P: Â¿Puedo agregar mÃ¡s transacciones despuÃ©s?**
R: SÃ­, re-ejecuta el ETL con el archivo actualizado.

**P: Â¿Los cÃ³digos de Ã¡rea cambian?**
R: Actualiza el JSON y re-ejecuta. El script es idempotente.

**P: Â¿QuÃ© pasa si hay duplicados en los datos?**
R: Se conservan en transacciones (son eventos Ãºnicos) y se eliminan en regiones.

---

## ğŸ“ Licencia y AutorÃ­a

**Proyecto:** ETL Casino - AnÃ¡lisis Regional  
**MetodologÃ­a:** Hefesto  
**PropÃ³sito:** Trabajo Final - Bases de Datos  
**AÃ±o:** 2025

---

## âœ… Checklist Pre-PresentaciÃ³n

- [ ] ETL ejecuta sin errores
- [ ] Datos procesados verificados
- [ ] Reportes generados en Excel
- [ ] Logs documentados
- [ ] AnÃ¡lisis ejecutivo completado
- [ ] GrÃ¡ficos preparados
- [ ] PresentaciÃ³n PowerPoint lista
- [ ] CÃ³digo documentado y comentado
- [ ] README actualizado
- [ ] Archivos de entrada/salida organizados

---

## ğŸš€ PrÃ³ximos Pasos

1. **VisualizaciÃ³n:** Crear grÃ¡ficos con Matplotlib/Plotly
2. **Dashboard:** Exportar a Power BI o Tableau
3. **API:** Servir datos via FastAPI/Flask
4. **AutomatizaciÃ³n:** Programar ETL automÃ¡tico (cron/scheduler)
5. **Machine Learning:** PredicciÃ³n de depÃ³sitos por regiÃ³n

